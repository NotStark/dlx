{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NotStark/dlx/blob/main/dlx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DLX** - A Project for downloading and uploading files to fast cloud storage providers"
      ],
      "metadata": {
        "id": "WBaX62VAun5J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üõ†Ô∏è Setup Environment\n",
        "\n",
        "\n",
        "# Update Silently\n",
        "!apt update -qq\n",
        "\n",
        "# Install required packages\n",
        "!apt install -y -qq aria2 ffmpeg mediainfo\n",
        "\n",
        "# Install ytdl from github\n",
        "!curl -L https://github.com/yt-dlp/yt-dlp/releases/latest/download/yt-dlp -o /usr/local/bin/yt-dlp\n",
        "!chmod +x /usr/local/bin/yt-dlp\n",
        "!yt-dlp --version\n",
        "\n",
        "# Install Python libraries\n",
        "!pip install -q --upgrade \\\n",
        "  tqdm \\\n",
        "  httpx \\\n",
        "  psutil \\\n",
        "  Kurigram \\\n",
        "  tgcrypto \\\n",
        "  natsort \\\n",
        "  google-api-python-client\n",
        "\n",
        "\n",
        "import os\n",
        "import re\n",
        "from google.colab import userdata\n",
        "from pathlib import Path\n",
        "from typing import Literal\n",
        "\n",
        "def get_secret(key):\n",
        "    try:\n",
        "        return userdata.get(key)\n",
        "    except Exception:\n",
        "        print(f\"{key} Not Found\")\n",
        "        return None\n",
        "\n",
        "# Load secrets\n",
        "TELEGRAM_BOT_TOKEN = get_secret('TELEGRAM_BOT_TOKEN')\n",
        "GOFILE_API_TOKEN = get_secret(\"GOFILE_API_TOKEN\")\n",
        "VIKINGFILE_USER_HASH = get_secret(\"VIKINGFILE_USER_HASH\")\n",
        "BUZZHEAVIER_USER_TOKEN = get_secret(\"BUZZHEAVIER_USER_TOKEN\")\n",
        "\n",
        "\n",
        "# Download directory\n",
        "DOWNLOADS_DIR = Path(os.path.abspath(\"downloads\"))\n",
        "DOWNLOADS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "\n",
        "# Utilities\n",
        "def format_size(bytes: float) -> str:\n",
        "    units = [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]\n",
        "    i = 0\n",
        "    while bytes >= 1024 and i < len(units) - 1:\n",
        "        bytes /= 1024\n",
        "        i += 1\n",
        "    return f\"{bytes:.2f} {units[i]}\"\n",
        "\n",
        "def sanitize_filename(name: str) -> str:\n",
        "    return re.sub(r'[^a-zA-Z0-9_\\-. ]', '_', name)\n",
        "\n",
        "\n",
        "def find_full_path(base_path_no_ext: str) -> str | None:\n",
        "    base = Path(base_path_no_ext)\n",
        "    parent_dir = base.parent or Path.cwd()\n",
        "    for file in parent_dir.glob(base.stem + \".*\"):\n",
        "        if file.is_file():\n",
        "            return str(file.resolve())\n",
        "    return None\n",
        "\n",
        "def detect_url_type(url: str) -> Literal[\"magnet\" , \"torrent\" , \"m3u8\" , \"direct\" , \"unknown\"]:\n",
        "    url = url.lower()\n",
        "    if url.startswith(\"magnet:\"):\n",
        "        return \"magnet\"\n",
        "    elif url.endswith(\".torrent\"):\n",
        "        return \"torrent\"\n",
        "    elif url.endswith(\".m3u8\"):\n",
        "        return \"m3u8\"\n",
        "    elif url.startswith((\"http://\", \"https://\", \"ftp://\")):\n",
        "        return \"direct\"\n",
        "    return \"unknown\"\n",
        "\n",
        "\n",
        "def is_downloaded(file_path: str) -> bool:\n",
        "  return file_path.exists() and file_path.stat().st_size > 0\n",
        "\n",
        "\n",
        "print(f\"‚úÖ Environment ready. Download path: {DOWNLOADS_DIR}\")\n"
      ],
      "metadata": {
        "id": "1Z8gHsaYvCew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ‚è¨ Universal Media Downloader\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "from pathlib import Path\n",
        "from urllib.parse import urlparse\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "DEFAULT_M3U8_FILE_HEIGHT = 1080\n",
        "\n",
        "def download_m3u8(\n",
        "    url: str,\n",
        "    output_file=None,\n",
        "    yt_dlp_args: str = \"\",\n",
        "    aria2c_args: str = \"\",\n",
        "    file_height: int = DEFAULT_M3U8_FILE_HEIGHT,\n",
        "    aria2c_connections: int = 16,\n",
        "    aria2c_chunk_size: str = \"2M\"\n",
        "):\n",
        "    fmt = f\"bv[height<={file_height}]+ba/best[height<={file_height}]\"\n",
        "\n",
        "    if not output_file:\n",
        "        output_file = str(DOWNLOADS_DIR / f\"file_{int(time.time())}\")\n",
        "\n",
        "    cmd = [\n",
        "        \"yt-dlp\",\n",
        "        \"-f\", fmt,\n",
        "        \"--downloader\", \"aria2c\",\n",
        "        \"--downloader-args\", f\"aria2c:-x {aria2c_connections} -k {aria2c_chunk_size} --summary-interval=1 --console-log-level=warn {aria2c_args}\",\n",
        "        \"-o\", f\"{output_file}.%(ext)s\",\n",
        "        url\n",
        "    ]\n",
        "\n",
        "    if yt_dlp_args:\n",
        "        cmd[1:1] = yt_dlp_args.split()\n",
        "\n",
        "    print(f\"\\n‚¨áÔ∏è Starting Download [{file_height}p max]:\\n    {' '.join(cmd)}\\n\")\n",
        "\n",
        "    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1)\n",
        "\n",
        "    output_file_basename = os.path.basename(output_file)\n",
        "    progress_bar = tqdm(total=100, desc=f\"‚öôÔ∏è Downloading: {output_file_basename}\", unit=\"%\",)\n",
        "    progress_regex = re.compile(r\"\\((\\d+)%\\)\")\n",
        "    last_value = 0\n",
        "\n",
        "    for line in proc.stdout:\n",
        "        line = line.strip()\n",
        "        match = progress_regex.search(line)\n",
        "        if match:\n",
        "            perc = int(match.group(1))\n",
        "            if perc >= last_value:\n",
        "                progress_bar.n = perc\n",
        "                progress_bar.refresh()\n",
        "                last_value = perc\n",
        "\n",
        "        if \"[FixupM3u8]\" in line:\n",
        "            progress_bar.set_description(\"üõ†Ô∏è Finalizing Video...\")\n",
        "            progress_bar.n = 95\n",
        "            progress_bar.refresh()\n",
        "\n",
        "    proc.wait()\n",
        "    progress_bar.n = 100\n",
        "    progress_bar.set_description(f\"‚úÖ Completed: {output_file_basename}\")\n",
        "    progress_bar.refresh()\n",
        "    progress_bar.close()\n",
        "\n",
        "    if proc.returncode == 0:\n",
        "        print(f\"\\n‚úÖ Download complete: {output_file_basename}\")\n",
        "        return find_full_path(output_file), True\n",
        "    else:\n",
        "        print(f\"\\n‚ùå Download failed (exit code {proc.returncode})\")\n",
        "        return None, False\n",
        "\n",
        "\n",
        "def download_media(\n",
        "    urls: list[str] = None,\n",
        "    folder: str = None,\n",
        "    batch_size: int = 5,\n",
        "    delay: int = 10,\n",
        "    m3u8_file_height=DEFAULT_M3U8_FILE_HEIGHT\n",
        "):\n",
        "\n",
        "    def download_single(url, out_dir, filename=None, m3u8_file_height=DEFAULT_M3U8_FILE_HEIGHT):\n",
        "        url_type = detect_url_type(url)\n",
        "        sanitized_filename = sanitize_filename(filename or f\"file_{int(time.time())}\")\n",
        "        output_path = out_dir / sanitized_filename\n",
        "\n",
        "        if is_downloaded(output_path):\n",
        "            print(f\"‚è≠Ô∏è Already exists: {filename}\")\n",
        "            return\n",
        "\n",
        "        if url_type in [\"magnet\", \"torrent\", \"direct\"]:\n",
        "            cmd = [\n",
        "                \"aria2c\",\n",
        "                f\"--dir={out_dir}\",\n",
        "                \"--enable-dht=true\",\n",
        "                \"--enable-peer-exchange=true\",\n",
        "                \"--bt-save-metadata=true\",\n",
        "                \"--console-log-level=notice\",\n",
        "                \"--summary-interval=1\",\n",
        "                \"--file-allocation=none\",\n",
        "                \"--seed-time=0\",\n",
        "                \"--bt-seed-unverified=true\",\n",
        "                url\n",
        "            ]\n",
        "\n",
        "            output_file_regex = re.compile(r\"(?:Download complete:|FILE:)\\s*(.+)\")\n",
        "            progress_regex = re.compile(r\"(\\d+)%\")\n",
        "\n",
        "            if filename:\n",
        "                cmd.insert(-1, f\"--out={filename}\")\n",
        "                final_output_name = filename\n",
        "                desc = f\"Downloading: {filename}\"\n",
        "            else:\n",
        "                final_output_name = None\n",
        "                desc = \"Downloading: [detecting...]\"\n",
        "\n",
        "            print(f\"‚¨áÔ∏è {desc}\")\n",
        "            proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "\n",
        "            pbar = tqdm(total=100, unit='%', desc=desc, leave=True)\n",
        "            for line in proc.stdout:\n",
        "                line = line.strip()\n",
        "                # print(line , end = \"\\n\")\n",
        "                _match = progress_regex.search(line)\n",
        "                if _match:\n",
        "                    perc = int(_match.group(1))\n",
        "                    pbar.n = perc\n",
        "                    pbar.refresh()\n",
        "\n",
        "                if not filename:\n",
        "                    fmatch = output_file_regex.search(line)\n",
        "                    if fmatch:\n",
        "                        final_output_name = Path(fmatch.group(1)).name\n",
        "                        pbar.set_description(f\"Downloading: {final_output_name}\")\n",
        "\n",
        "            pbar.n = 100\n",
        "            pbar.refresh()\n",
        "            pbar.close()\n",
        "            proc.wait()\n",
        "\n",
        "\n",
        "            if proc.returncode == 0:\n",
        "                print(f\"\\n‚úÖ Download complete: {final_output_name}\")\n",
        "                return str(out_dir / final_output_name), True\n",
        "            else:\n",
        "                print(f\"\\n‚ùå Download failed (exit code {proc.returncode})\")\n",
        "                return None , False\n",
        "\n",
        "        elif url_type == \"m3u8\":\n",
        "            return download_m3u8(url, output_file=str(output_path), file_height=m3u8_file_height)\n",
        "        else:\n",
        "            print(\"‚ùå Unsupported URL type.\")\n",
        "            return None, False\n",
        "\n",
        "\n",
        "    out_dir = Path(folder) if folder else DOWNLOADS_DIR\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if urls is None:\n",
        "        url = input(\"üîó Enter the media URL: \").strip()\n",
        "        filename = input(\"üìÅ Enter output filename (optional): \").strip() or None\n",
        "        if detect_url_type(url) == \"m3u8\":\n",
        "            res = input(\"üßê Enter resolution (360, 480, 720, 1080): \").strip()\n",
        "            res = int(res) if res.isdigit() else DEFAULT_M3U8_FILE_HEIGHT\n",
        "            m3u8_file_height = res\n",
        "\n",
        "        return download_single(url, out_dir, filename, m3u8_file_height)\n",
        "    else:\n",
        "        results = []\n",
        "        total = len(urls)\n",
        "        for i in range(0, total, batch_size):\n",
        "            batch = urls[i:i + batch_size]\n",
        "            print(f\"üöÄ Batch {i // batch_size + 1} of {((total - 1) // batch_size) + 1}\")\n",
        "\n",
        "            threads = []\n",
        "            batch_results = [None] * len(batch)\n",
        "\n",
        "            def thread_wrapper(idx, url):\n",
        "                path, success = download_single(url, out_dir, None, m3u8_file_height)\n",
        "                batch_results[idx] = {\n",
        "                    \"url\": url,\n",
        "                    \"path\": path,\n",
        "                    \"success\": success\n",
        "                }\n",
        "\n",
        "            for idx, url in enumerate(batch):\n",
        "                t = threading.Thread(target=thread_wrapper, args=(idx, url))\n",
        "                t.start()\n",
        "                threads.append(t)\n",
        "\n",
        "            for t in threads:\n",
        "                t.join()\n",
        "\n",
        "            results.extend(batch_results)\n",
        "\n",
        "            if i + batch_size < total:\n",
        "                print(f\"‚è≥ Sleeping {delay}s before next batch...\")\n",
        "                time.sleep(delay)\n",
        "\n",
        "        print(\"\\nüéâ All downloads processed.\")\n",
        "        return results\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  print(download_media())"
      ],
      "metadata": {
        "id": "vMAWxc7Vwgf_",
        "cellView": "code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üì∫ yt-dlp Downloader {\"form-width\":\"35%\"}\n",
        "# @markdown See [Supported sites](https://github.com/yt-dlp/yt-dlp/blob/master/supportedsites.md)\n",
        "\n",
        "\n",
        "import os\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "VIDEO_QUALITIES = [2160, 1440, 1080, 720, 480, 360, 240, 144]\n",
        "AUDIO_FORMATS = [\"mp3\", \"m4a\", \"opus\", \"wav\", \"flac\"]\n",
        "\n",
        "def ytdlp_downloader(\n",
        "    url: str,\n",
        "    fmt: str,\n",
        "    container: str | None,\n",
        "    is_playlist: bool,\n",
        "):\n",
        "    if is_playlist:\n",
        "        out_tpl = str(DOWNLOADS_DIR / \"%(playlist_title)s/%(title)s.%(ext)s\")\n",
        "    else:\n",
        "        out_tpl = str(DOWNLOADS_DIR / \"%(title)s.%(ext)s\")\n",
        "\n",
        "    cmd = [\n",
        "        \"yt-dlp\",\n",
        "        \"-f\", fmt,\n",
        "        \"-o\", out_tpl,\n",
        "        \"--no-part\",\n",
        "        \"--progress\",\n",
        "        \"--progress-template\", \"%(info.playlist_index)s %(progress._default_template)s\",\n",
        "        \"--no-continue\",\n",
        "        \"--yes-playlist\" if is_playlist else \"--no-playlist\",\n",
        "        \"--newline\",\n",
        "        url\n",
        "    ]\n",
        "\n",
        "    if fmt == \"bestaudio\" and container:\n",
        "        cmd += [\n",
        "            \"--extract-audio\",\n",
        "            \"--audio-format\", container,\n",
        "            \"--audio-quality\", \"0\"\n",
        "        ]\n",
        "    elif container and fmt != \"bestaudio\":\n",
        "        cmd.insert(3, \"--merge-output-format\")\n",
        "        cmd.insert(4, container)\n",
        "\n",
        "    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
        "    for line in proc.stdout:\n",
        "        print(line, end=\"\")\n",
        "\n",
        "    proc.wait()\n",
        "\n",
        "    if proc.returncode == 0:\n",
        "        print(\"‚úÖ Download complete! üéâ\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è yt-dlp exited with code {proc.returncode}\")\n",
        "\n",
        "\n",
        "def prompt_for_format() -> tuple[str, str]:\n",
        "    choice = input(\"üéß Audio or üì∫ Video? (audio/video): \").strip().lower()\n",
        "    if choice.startswith(\"a\"):\n",
        "        print(\"üéº Supported audio formats: \" + \", \".join(AUDIO_FORMATS))\n",
        "        ext = input(\"üéöÔ∏è Desired audio format (default=mp3): \").strip().lower() or \"mp3\"\n",
        "        if ext not in AUDIO_FORMATS:\n",
        "            print(\"‚ùå Invalid format. Using default mp3.\")\n",
        "            ext = \"mp3\"\n",
        "        return \"bestaudio\", ext\n",
        "    elif choice.startswith(\"v\"):\n",
        "        ext = input(\"üéöÔ∏è Desired resolution container (mp4/mpv/webm, default=mp4): \").strip().lower() or \"mp4\"\n",
        "        if ext not in [\"mp4\", \"webm\", \"mkv\"]:\n",
        "            print(\"‚ùå Unsupported container. Using default mp4.\")\n",
        "            ext = \"mp4\"\n",
        "        print(\"üî¢ Resolutions: \" + \", \".join(str(r) for r in VIDEO_QUALITIES))\n",
        "        while True:\n",
        "            r = input(\"üéöÔ∏è Desired max height (e.g. 1080, or blank for best): \").strip()\n",
        "            if r == \"\":\n",
        "                return \"bv+ba/best\", ext\n",
        "            if r.isdigit() and int(r) in VIDEO_QUALITIES:\n",
        "                res = int(r)\n",
        "                fmt = f\"bv[height<={res}]+ba/best[height<={res}]\"\n",
        "                return fmt, ext\n",
        "            print(\"‚ùå Invalid resolution.\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚ùå Invalid choice.\")\n",
        "        return prompt_for_format()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mode = input(\"‚û°Ô∏è Mode? (single/playlist): \").strip().lower()\n",
        "    url = input(\"üîó URL: \").strip()\n",
        "\n",
        "    if mode.startswith(\"s\") or mode.startswith(\"p\"):\n",
        "        fmt, ext = prompt_for_format()\n",
        "        ytdlp_downloader(\n",
        "            url,\n",
        "            fmt,\n",
        "            ext,\n",
        "            is_playlist=mode.startswith(\"p\"),\n",
        "        )\n",
        "    else:\n",
        "        print(\"‚ùå Unknown mode.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    print(f\"\\n‚úÖ All done! Files saved in {DOWNLOADS_DIR}\")\n"
      ],
      "metadata": {
        "id": "BP9k5CzrFaHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ‚¨ÜÔ∏è Cloud Uploader (Drive, Gofile, VikingFile, BuzzHeavier) {\"form-width\":\"35%\"}\n",
        "\n",
        "\n",
        "import os\n",
        "import time\n",
        "import gc\n",
        "from abc import ABC, abstractmethod\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import httpx\n",
        "import json\n",
        "import math\n",
        "from google.colab import drive, auth\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from tqdm.notebook import tqdm\n",
        "from tqdm.utils import CallbackIOWrapper\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "if not GOFILE_API_TOKEN:\n",
        "    GOFILE_API_TOKEN = \"\"  # @param {type:\"string\", placeholder:\"(Optional) Enter your optional Gofile API Token\"}\n",
        "\n",
        "if not VIKINGFILE_USER_HASH:\n",
        "    VIKINGFILE_USER_HASH = \"\"  # @param {type:\"string\", placeholder:\"(Optional) Enter your optional VikingFile User Hash\"}\n",
        "\n",
        "if not BUZZHEAVIER_USER_TOKEN:\n",
        "    BUZZHEAVIER_USER_TOKEN = \"\"  # @param {type:\"string\", placeholder:\"(Optional) Enter your optional BuzzHeavier User Token\"}\n",
        "\n",
        "\n",
        "\n",
        "class BaseUploader(ABC):\n",
        "    @abstractmethod\n",
        "    def upload(self, remote_path: str) -> None:\n",
        "        \"\"\"Uploads file or folder\"\"\"\n",
        "\n",
        "# Gdrive Uploader\n",
        "class GoogleDriveUploader(BaseUploader):\n",
        "    DRIVE_MOUNT = '/content/drive'\n",
        "    DRIVE_SERVICE = ('drive', 'v3')\n",
        "\n",
        "    def __init__(self, base_folder: str = ''):\n",
        "        self.base_folder = base_folder\n",
        "        self.service = None\n",
        "        self.folder_map = {'': None}\n",
        "\n",
        "    def _authenticate(self):\n",
        "        auth.authenticate_user()\n",
        "        drive.mount(self.DRIVE_MOUNT, force_remount=True)\n",
        "        self.service = build(*self.DRIVE_SERVICE, cache_discovery=False)\n",
        "\n",
        "    def _ensure_folder(self, parts: list[str]) -> str | None:\n",
        "        parent = None\n",
        "        for i, part in enumerate(parts):\n",
        "            key = '/'.join(parts[:i+1])\n",
        "            if key in self.folder_map:\n",
        "                parent = self.folder_map[key]\n",
        "                continue\n",
        "            q = f\"name='{part}' and mimeType='application/vnd.google-apps.folder' and trashed=false\"\n",
        "            if parent:\n",
        "                q += f\" and '{parent}' in parents\"\n",
        "            resp = self.service.files().list(q=q, fields='files(id)').execute()\n",
        "            if resp['files']:\n",
        "                fid = resp['files'][0]['id']\n",
        "            else:\n",
        "                body = {'name': part, 'mimeType': 'application/vnd.google-apps.folder'}\n",
        "                if parent: body['parents'] = [parent]\n",
        "                fid = self.service.files().create(body=body, fields='id').execute()['id']\n",
        "            self.folder_map[key] = fid\n",
        "            parent = fid\n",
        "        return parent\n",
        "\n",
        "    def _upload_file(self, path: str, parent_id: str | None) -> None:\n",
        "        fname = os.path.basename(path)\n",
        "        file_size = os.path.getsize(path)\n",
        "        media = MediaFileUpload(path, resumable=True)\n",
        "        meta = {'name': fname, **({'parents':[parent_id]} if parent_id else {})}\n",
        "\n",
        "        request = self.service.files().create(body=meta, media_body=media, fields='id')\n",
        "\n",
        "        pbar = tqdm(total=file_size, unit='B', unit_scale=True, desc=f\"Uploading {fname}\")\n",
        "        response = None\n",
        "        while response is None:\n",
        "            status, response = request.next_chunk()\n",
        "            if status:\n",
        "                pbar.update(status.resumable_progress - pbar.n)\n",
        "\n",
        "\n",
        "        if pbar.n < file_size:\n",
        "            pbar.update(file_size - pbar.n)\n",
        "\n",
        "        pbar.close()\n",
        "\n",
        "\n",
        "        file_id = response.get('id')\n",
        "        print(f\"‚úÖ {fname} ‚Üí https://drive.google.com/file/d/{file_id}/view\")\n",
        "        return file_id\n",
        "\n",
        "    def upload(self, remote_path: str) -> None:\n",
        "        self._authenticate()\n",
        "        target = os.path.join(DOWNLOADS_DIR, remote_path)\n",
        "        if not os.path.exists(target):\n",
        "            print(f\"‚ùå Not found: {target}\")\n",
        "            return\n",
        "        if os.path.isfile(target):\n",
        "            self._upload_file(target, None)\n",
        "            return\n",
        "        for root, _, files in os.walk(target):\n",
        "            rel = os.path.relpath(root, DOWNLOADS_DIR)\n",
        "            parts = [] if rel == '.' else rel.split(os.sep)\n",
        "            parts = ([self.base_folder] if self.base_folder else []) + parts\n",
        "            parent = self._ensure_folder(parts)\n",
        "            print(f\"üìÇ {'/'.join(parts) or '<root>'}\")\n",
        "            for f in files:\n",
        "                self._upload_file(os.path.join(root, f), parent)\n",
        "            print(\"‚è∏Ô∏è Sleeping 10‚Äâs...\")\n",
        "            time.sleep(10)\n",
        "        print(\"‚úÖ Google Drive upload done\")\n",
        "\n",
        "\n",
        "# GoFile uploader\n",
        "class GoFileUploader(BaseUploader):\n",
        "    API_CREATE_FOLDER = 'https://api.gofile.io/contents/createFolder'\n",
        "    API_UPLOAD       = 'https://upload.gofile.io/uploadFile'\n",
        "\n",
        "    def __init__(self, token: str | None = None , root_id: str | None = None):\n",
        "        self.headers = {'Authorization': f'Bearer {token}'}\n",
        "        self.root_id = root_id\n",
        "        self.httpxClient = httpx.Client(headers=self.headers)\n",
        "\n",
        "    def _new_folder(self, parent: str, name: str) -> str:\n",
        "        resp = self.httpxClient.post(self.API_CREATE_FOLDER,\n",
        "                          json={'parentFolderId': parent, 'folderName': name})\n",
        "        data = resp.json()\n",
        "        if data['status']=='ok':\n",
        "            fid = data['data']['id']\n",
        "            print(f\"üìÇ Created {name} (ID:{fid})\")\n",
        "            return fid\n",
        "        raise RuntimeError(data.get('message'))\n",
        "\n",
        "    def _get_root_folder_id(self) -> str:\n",
        "        resp = self.httpxClient.get(\"https://api.gofile.io/accounts/getid\")\n",
        "        data = resp.json()\n",
        "        if data[\"status\"] == \"ok\":\n",
        "            account_id =  data[\"data\"][\"id\"]\n",
        "            resp = self.httpxClient.get(f\"https://api.gofile.io/accounts/{account_id}\")\n",
        "            data = resp.json()\n",
        "            if data[\"status\"] == \"ok\":\n",
        "              return data[\"data\"][\"rootFolder\"]\n",
        "            else:\n",
        "              raise RuntimeError(\"Failed to get root folder ID\")\n",
        "        else:\n",
        "            raise RuntimeError(\"Failed to get account ID\")\n",
        "\n",
        "    def _upload_file(self, path: str, folder: str | None) -> None:\n",
        "        fname = os.path.basename(path)\n",
        "        size = os.path.getsize(path)\n",
        "\n",
        "        with open(path, \"rb\") as f, \\\n",
        "             tqdm(total=size, unit=\"B\", unit_scale=True, desc=f\"‚¨ÜÔ∏è {fname}\") as bar:\n",
        "\n",
        "            wrapped_file = CallbackIOWrapper(bar.update, f, \"read\")\n",
        "            files = {\"file\": (fname, wrapped_file)}\n",
        "            data = {'folderId': folder} if folder else {}\n",
        "            result = None\n",
        "\n",
        "            with self.httpxClient as client:\n",
        "                with client.stream(\n",
        "                    \"POST\",\n",
        "                    self.API_UPLOAD,\n",
        "                    files=files,\n",
        "                    data=data,\n",
        "                    timeout=None\n",
        "                ) as response:\n",
        "                    response.raise_for_status()\n",
        "                    text = response.read()\n",
        "                    result = json.loads(text)\n",
        "\n",
        "        if result.get(\"status\") == \"ok\":\n",
        "            print(f\"‚úÖ {fname} ‚Üí {result['data']['downloadPage']}\")\n",
        "        else:\n",
        "            raise RuntimeError(result.get(\"message\"))\n",
        "\n",
        "\n",
        "\n",
        "    def upload(self, remote_path: str) -> None:\n",
        "\n",
        "        tgt = os.path.join(DOWNLOADS_DIR, remote_path)\n",
        "\n",
        "        if not os.path.exists(tgt): print(f\"‚ùå Not found: {tgt}\"); return\n",
        "\n",
        "        if not self.token:\n",
        "            raise RuntimeError(\"GoFile token is missing\")\n",
        "\n",
        "\n",
        "        if not self.root_id:\n",
        "            self.root_id = self._get_root_folder_id()\n",
        "            print(f\"üìÅ Root folder ID: {self.root_id}\")\n",
        "\n",
        "        folder_map = {'': self.root_id}\n",
        "\n",
        "        if os.path.isfile(tgt): return self._upload_file(tgt, None)\n",
        "        for root, _, files in os.walk(tgt):\n",
        "            rel = os.path.relpath(root, DOWNLOADS_DIR)\n",
        "            parent_key = os.path.dirname(rel)\n",
        "            parent = self.folder_map.get(parent_key)\n",
        "            if rel not in self.folder_map:\n",
        "                self.folder_map[rel] = self._new_folder(parent, os.path.basename(root))\n",
        "            print(f\"üìÇ Folder: {root}\")\n",
        "            for f in files:\n",
        "                self._upload_file(os.path.join(root, f), self.folder_map[rel])\n",
        "            print(\"‚è∏Ô∏è Sleeping 10‚Äâs...\")\n",
        "            time.sleep(10)\n",
        "        print(\"‚úÖ GoFile upload done\")\n",
        "\n",
        "\n",
        "\n",
        "# VikingFile Uploader\n",
        "class VikingFileUploader:\n",
        "    API_BASE = 'https://vikingfile.com/api'\n",
        "\n",
        "    def __init__(self, user_hash: str | None = None):\n",
        "        self.user = user_hash\n",
        "        self.client = httpx.Client(\n",
        "            timeout=httpx.Timeout(connect=10.0, read=60.0, write=300.0, pool=10.0)\n",
        "        )\n",
        "\n",
        "    def _init_multipart(self, size: int):\n",
        "        resp = self.client.post(f\"{self.API_BASE}/get-upload-url\", data={'size': size})\n",
        "        resp.raise_for_status()\n",
        "        info = resp.json()\n",
        "        return info['uploadId'], info['key'], info['partSize'], info['urls']\n",
        "\n",
        "    def _complete(self, key: str, uid: str, name: str, etags: list):\n",
        "        payload = {\n",
        "            'key': key,\n",
        "            'uploadId': uid,\n",
        "            'name': name,\n",
        "            'user': self.user\n",
        "        }\n",
        "        for i, (num, tag) in enumerate(etags):\n",
        "            payload[f\"parts[{i}][PartNumber]\"] = num\n",
        "            payload[f\"parts[{i}][ETag]\"] = tag\n",
        "\n",
        "        resp = self.client.post(f\"{self.API_BASE}/complete-upload\", data=payload)\n",
        "        resp.raise_for_status()\n",
        "        return resp.json().get('url')\n",
        "\n",
        "\n",
        "    def _upload_part(self, url, file_path, offset, size, desc=None, thread_id=None):\n",
        "        with open(file_path, 'rb') as f:\n",
        "            f.seek(offset)\n",
        "            chunk_data = f.read(size)\n",
        "\n",
        "        pbar = tqdm(total=size, unit='B', unit_scale=True, desc=desc, position=thread_id, leave=True)\n",
        "        buffer = BytesIO(chunk_data)\n",
        "        wrapped = CallbackIOWrapper(pbar.update, buffer, \"read\")\n",
        "\n",
        "        def stream_chunk(chunk_size=100 * 1024 * 1024):\n",
        "            while True:\n",
        "                data = wrapped.read(chunk_size)\n",
        "                if not data:\n",
        "                    break\n",
        "                yield data\n",
        "\n",
        "\n",
        "\n",
        "        headers = {'Content-Length': str(size)}\n",
        "        resp = self.client.put(url, content=stream_chunk(), headers=headers)\n",
        "        resp.raise_for_status()\n",
        "        pbar.close()\n",
        "        del chunk_data\n",
        "        del buffer\n",
        "        gc.collect()\n",
        "        return resp.headers.get('ETag', '')\n",
        "\n",
        "\n",
        "\n",
        "    def _upload_file(self, full_path: str):\n",
        "        size = os.path.getsize(full_path)\n",
        "        filename = os.path.basename(full_path)\n",
        "        print(f\"\\nüîÑ Init: {filename} ({size} bytes)\")\n",
        "\n",
        "        uid, key, part_size, urls = self._init_multipart(size)\n",
        "        etags = []\n",
        "\n",
        "        usable_ram = 10 * 1024**3  # 10 GiB\n",
        "        memory_per_part = part_size * 1.2\n",
        "        max_workers = max(1, math.floor(usable_ram / memory_per_part))\n",
        "        print(f\"üß† Auto workers: {max_workers} (based on {format_size(part_size)} part size)\")\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "            futures = []\n",
        "            offset = 0\n",
        "\n",
        "            for i, url in enumerate(urls):\n",
        "                current_part_size = min(part_size, size - offset)\n",
        "                if current_part_size <= 0:\n",
        "                    break\n",
        "                fut = ex.submit(self._upload_part, url, full_path, offset, current_part_size, f\"Part {i+1}\", i)\n",
        "                futures.append((i + 1, fut))\n",
        "                offset += current_part_size\n",
        "\n",
        "            for part_number, fut in tqdm(futures, desc=f\"‚¨ÜÔ∏è ETags {filename}\", unit=\"etag\",position=99):\n",
        "                etag = fut.result()\n",
        "                etags.append((part_number, etag))\n",
        "\n",
        "        etags.sort(key=lambda x: x[0])\n",
        "        result = self._complete(key, uid, filename, etags)\n",
        "        print(f\"‚úÖ Done: {result}\\n\")\n",
        "\n",
        "    def upload(self, remote_path: str):\n",
        "        path = os.path.join(DOWNLOADS_DIR, remote_path)\n",
        "        if not os.path.exists(path):\n",
        "            print(f\"‚ùå Not found: {path}\")\n",
        "            return\n",
        "\n",
        "        if not self.user_hash:\n",
        "            raise RuntimeError(\"VikingFile user hash is missing\")\n",
        "\n",
        "        if os.path.isfile(path): return self._upload_file(path)\n",
        "        print(f\"üìÅ Uploading folder: {path}\")\n",
        "        for rd, _, files in os.walk(path):\n",
        "            for f in files:\n",
        "                self._upload_file(os.path.join(rd, f))\n",
        "            print(\"‚è∏Ô∏è Sleep 10‚Äâs...\")\n",
        "            time.sleep(10)\n",
        "        print(\"‚úÖ VikingFile Upload done\")\n",
        "\n",
        "\n",
        "# BuzzHeavier uploader\n",
        "class BuzzHeavierUploader(BaseUploader):\n",
        "\n",
        "    BUZZ_API_ROOT = 'https://buzzheavier.com/api'\n",
        "    BUZZ_UPLOAD_ROOT = 'https://w.buzzheavier.com'\n",
        "\n",
        "    def __init__(self, token: str | None = None , root_id: str | None = None):\n",
        "        self.headers = {'Authorization': f'Bearer {token}'}\n",
        "        self.root_id = root_id\n",
        "        self.httpxClient = httpx.Client(headers=self.headers)\n",
        "\n",
        "    def _create_folder(self, name: str, parent: str | None) -> str:\n",
        "        data = {'name': name, 'parentId': parent or self.root}\n",
        "        resp = httpx.post(f\"{self.BUZZ_API_ROOT}/fs\", headers=self.headers, json=data)\n",
        "        info = resp.json()\n",
        "        if info.get('status')!='ok': raise RuntimeError(info.get('message'))\n",
        "        fid = info['data']['id']; print(f\"üìÇ New: {name} (ID:{fid})\"); return fid\n",
        "\n",
        "\n",
        "    def _get_root_folder_id(self) -> str:\n",
        "        resp = httpx.get(f\"{self.BUZZ_API_ROOT}/fs\", headers=self.headers)\n",
        "        info = resp.json()\n",
        "        if info.get('status')!='ok': raise RuntimeError(info.get('message'))\n",
        "        return info['data']['id']\n",
        "\n",
        "    def _upload_file(self, path: str, parent: str | None) -> None:\n",
        "        fname = os.path.basename(path)\n",
        "        size = os.path.getsize(path)\n",
        "        url = f\"{self.BUZZ_UPLOAD_ROOT}/{(parent + '/') if parent else ''}{fname}\"\n",
        "        params = {'folderId': parent} if parent else {}\n",
        "\n",
        "        with open(path, \"rb\") as f, \\\n",
        "             tqdm(total=size, unit=\"B\", unit_scale=True, desc=f\"‚¨ÜÔ∏è {fname}\") as bar:\n",
        "\n",
        "            wrapped_file = CallbackIOWrapper(bar.update, f, \"read\")\n",
        "\n",
        "            with httpx.Client(headers=self.headers, timeout=None) as client:\n",
        "                with client.stream(\"PUT\", url, params=params, content=wrapped_file) as response:\n",
        "                    response.raise_for_status()\n",
        "                    text = response.read()\n",
        "                    result = json.loads(text)\n",
        "                    print(result)\n",
        "\n",
        "\n",
        "    def upload(self, remote_path: str) -> None:\n",
        "\n",
        "        pth = os.path.join(DOWNLOADS_DIR, remote_path)\n",
        "\n",
        "        if not os.path.exists(pth): print(f\"‚ùå Not found: {pth}\"); return\n",
        "\n",
        "\n",
        "        if not self.token:\n",
        "            raise RuntimeError(\"Buzzheavie token is missing\")\n",
        "\n",
        "        if not self.root_id:\n",
        "            self.root_id = self._get_root_folder_id()\n",
        "            print(f\"üìÅ Root folder ID: {self.root_id}\")\n",
        "\n",
        "\n",
        "        if os.path.isfile(pth): return self._upload_file(pth, None)\n",
        "        top = self._create_folder(os.path.basename(pth), None)\n",
        "        for rd, _, files in os.walk(pth):\n",
        "            rel = os.path.relpath(rd, pth)\n",
        "            parent = top if rel==' .' else self._create_folder(rel, top)\n",
        "            for f in files:\n",
        "                self._upload_file(os.path.join(rd,f), parent)\n",
        "\n",
        "            print(\"‚è∏Ô∏è Sleep 10‚Äâs...\")\n",
        "            time.sleep(10)\n",
        "        print(\"‚úÖ BuzzHeavier done\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "    services = [\n",
        "    ('üóÇÔ∏è GoFile',        GoFileUploader(GOFILE_API_TOKEN)),\n",
        "    ('‚òÅÔ∏è Google Drive',  GoogleDriveUploader()),\n",
        "    ('‚öîÔ∏è VikingFile',    VikingFileUploader(VIKINGFILE_USER_HASH)),\n",
        "    ('üì° BuzzHeavier',   BuzzHeavierUploader(BUZZHEAVIER_USER_TOKEN)),\n",
        "    ]\n",
        "    print(\"‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì\")\n",
        "    for i,(n,_) in enumerate(services): print(f\"[{i}] {n}\")\n",
        "    print(\"‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì‚Äì\")\n",
        "    idx = int(input(\"‚û°Ô∏è Select service index: \").strip())\n",
        "    path = input(\"üìÇ File/folder path: \").strip()\n",
        "    services[idx][1].upload(path)\n"
      ],
      "metadata": {
        "id": "ylo09odzFXCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üì© Telegram Media Downloader\n",
        "\n",
        "import re\n",
        "import os\n",
        "import asyncio\n",
        "import requests\n",
        "from tqdm.notebook import tqdm\n",
        "from pyrogram import Client\n",
        "\n",
        "\n",
        "if not TELEGRAM_BOT_TOKEN:\n",
        "    TELEGRAM_BOT_TOKEN = \"\"  # @param {type:\"string\", placeholder:\"(Optional) Enter your Telegram bot token\"}\n",
        "\n",
        "\n",
        "def parse_link(link: str) -> tuple[int | str , int]:\n",
        "    m = re.match(\n",
        "        r\"https?://(?:t\\.me|telegram\\.me|telegram\\.dog|telegram\\.org)/(?:c/)?(?P<chat>[^/]+)/(?P<msg_id>\\d+)\",\n",
        "        link.strip().lower()\n",
        "    )\n",
        "    if not m:\n",
        "        return None\n",
        "    chat = m.group(\"chat\")\n",
        "    msg_id = int(m.group(\"msg_id\"))\n",
        "    if chat.isdigit() and int(chat) > 0:\n",
        "        chat = int(f\"-100{chat}\")\n",
        "    return (chat, msg_id)\n",
        "\n",
        "\n",
        "async def download_one(app: Client, chat: int | str, msg_id: int) -> None:\n",
        "    try:\n",
        "        message = await app.get_messages(chat, msg_id)\n",
        "\n",
        "        if not message:\n",
        "            print(f\"‚ùå Message not found: {chat}/{msg_id}\")\n",
        "            return\n",
        "\n",
        "        file_name, file_size = (\n",
        "              (message.video.file_name, message.video.file_size) if message.video else\n",
        "              (message.audio.file_name, message.audio.file_size) if message.audio else\n",
        "              (message.document.file_name, message.document.file_size) if message.document else\n",
        "              (\"Photo\", message.photo.file_size) if message.photo else\n",
        "              (None, None)\n",
        "        )\n",
        "\n",
        "        if not file_size:\n",
        "            print(f\"‚ùå Unknown Media type: {chat}/{msg_id}\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\nüì• Downloading: {file_name}\")\n",
        "\n",
        "        with tqdm(total=file_size, unit=\"B\", unit_scale=True, desc=file_name, position=0, leave=True) as pbar:\n",
        "            def progress(current, total):\n",
        "                pbar.total = total\n",
        "                pbar.update(current - pbar.n)\n",
        "\n",
        "            file_path = await app.download_media(\n",
        "                message,\n",
        "                file_name=os.path.join(DOWNLOADS_DIR, file_name),\n",
        "                progress=progress\n",
        "            )\n",
        "\n",
        "        print(f\"‚úÖ Done: {file_name} ‚Üí {file_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚≠ï Error for {chat}/{msg_id}: {e}\")\n",
        "\n",
        "async def telegram_media_downloader():\n",
        "\n",
        "    links = input(\"üîó Enter Telegram message links (comma-separated):\\n\").split(\",\")\n",
        "\n",
        "    parsed_links = [parse_link(link) for link in links]\n",
        "    valid_links = [p for p in parsed_links if p]\n",
        "\n",
        "    if not valid_links:\n",
        "        print(\"‚ùå No valid links found.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    if not TELEGRAM_BOT_TOKEN:\n",
        "        raise RuntimeError(\"Telegram bot token is missing\")\n",
        "\n",
        "    app = Client(\n",
        "        \"ddl_bot\",\n",
        "        bot_token=TELEGRAM_BOT_TOKEN,\n",
        "        api_id=26684954,\n",
        "        api_hash=\"a709a6225180f08416b5d9effd1c9fd1\",\n",
        "        in_memory=True\n",
        "    )\n",
        "\n",
        "    await app.start()\n",
        "\n",
        "    try:\n",
        "        # Bots can't download in parallel\n",
        "        for chat, msg_id in valid_links:\n",
        "            await download_one(app, chat, msg_id)\n",
        "    finally:\n",
        "        await app.stop()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    await telegram_media_downloader()\n"
      ],
      "metadata": {
        "id": "Q7OtkL8LxmBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üé¨ AnimePahe Downloader\n",
        "\n",
        "import httpx\n",
        "import time\n",
        "import re\n",
        "from urllib.parse import quote, urlparse, parse_qs\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "\n",
        "\n",
        "BASE_URL = \"https://anime.apex-cloud.workers.dev\"\n",
        "KWIK_URL = \"https://access-kwik.apex-cloud.workers.dev/\"\n",
        "AUTH_TOKEN = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.e30.O0FKaqhJjEZgCAVfZoLz6Pjd7Gs9Kv6qi0P8RyATjaE\"\n",
        "RETRY_COUNT = 2\n",
        "RETRY_DELAY = 1.5\n",
        "RATE_LIMIT_BATCH = 5\n",
        "RATE_LIMIT_DELAY = 5\n",
        "\n",
        "\n",
        "def make_request(url, method=\"GET\", params=None, json_data=None, retries=RETRY_COUNT):\n",
        "\n",
        "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
        "\n",
        "    for attempt in range(1, retries + 1):\n",
        "        try:\n",
        "            if method.upper() == \"GET\":\n",
        "                response = httpx.get(url, params=params, headers=headers, timeout=15)\n",
        "            else:\n",
        "                response = httpx.post(url, json=json_data, headers=headers, timeout=15)\n",
        "\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except (httpx.RequestError, httpx.HTTPStatusError) as e:\n",
        "            if attempt == retries:\n",
        "                print(f\"‚ùå Error after {retries} attempts: {e}\")\n",
        "                raise\n",
        "            print(f\"‚ö†Ô∏è Attempt {attempt} failed: {e}. Retrying in {RETRY_DELAY}s...\")\n",
        "            time.sleep(RETRY_DELAY)\n",
        "\n",
        "def search_anime(query):\n",
        "    params = {\"method\": \"search\", \"query\": query}\n",
        "    return make_request(BASE_URL, params=params)\n",
        "\n",
        "def get_series_info(session_id, page=1):\n",
        "    params = {\"method\": \"series\", \"session\": session_id, \"page\": page}\n",
        "    return make_request(BASE_URL, params=params)\n",
        "\n",
        "def get_episode_links(series_session, episode_session):\n",
        "    params = {\"method\": \"episode\", \"session\": series_session, \"ep\": episode_session}\n",
        "    return make_request(BASE_URL, params=params)\n",
        "\n",
        "def resolve_kwik_link(kwik_url):\n",
        "    payload = {\n",
        "        \"service\": \"kwik\",\n",
        "        \"action\": \"fetch\",\n",
        "        \"content\": {\"kwik\": kwik_url},\n",
        "        \"auth\": AUTH_TOKEN\n",
        "    }\n",
        "    return make_request(KWIK_URL, method=\"POST\", json_data=payload)\n",
        "\n",
        "\n",
        "def select_best_quality(links):\n",
        "    def get_resolution(link):\n",
        "        match = re.search(r'(\\d+)p', link.get(\"name\", \"0p\"), re.IGNORECASE)\n",
        "        return int(match.group(1)) if match else 0\n",
        "\n",
        "    if not links:\n",
        "        return None\n",
        "\n",
        "    return max(links, key=get_resolution)\n",
        "\n",
        "def parse_expiry_time(url):\n",
        "    query_params = parse_qs(urlparse(url).query)\n",
        "    expires_list = query_params.get(\"expires\")\n",
        "\n",
        "    if not expires_list or not expires_list[0].isdigit():\n",
        "        return \"Unknown\"\n",
        "\n",
        "    try:\n",
        "        expiry_time = datetime.fromtimestamp(int(expires_list[0]))\n",
        "        return expiry_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    except (ValueError, TypeError):\n",
        "        return \"Unknown\"\n",
        "\n",
        "\n",
        "def validate_episode_range(input_str, max_episodes):\n",
        "\n",
        "    input_str = input_str.strip().lower()\n",
        "\n",
        "\n",
        "    if input_str in [\"all\", \"a\"]:\n",
        "        return list(range(1, max_episodes + 1))\n",
        "\n",
        "\n",
        "    if input_str.isdigit():\n",
        "        ep_num = int(input_str)\n",
        "        if 1 <= ep_num <= max_episodes:\n",
        "            return [ep_num]\n",
        "        else:\n",
        "            print(f\"‚ùå Episode {ep_num} is out of range (1-{max_episodes})\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    if \"-\" in input_str:\n",
        "        try:\n",
        "            start, end = map(int, input_str.split(\"-\"))\n",
        "            if start <= end and 1 <= start <= max_episodes and 1 <= end <= max_episodes:\n",
        "                return list(range(start, end + 1))\n",
        "            else:\n",
        "                print(f\"‚ùå Episode range {start}-{end} is invalid (must be between 1-{max_episodes})\")\n",
        "                return None\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "    print(\"‚ùå Invalid input format. Use 'all', a single number, or a range (e.g., '1-5')\")\n",
        "    return None\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Step 1: Search for anime\n",
        "    print(\"üîç Anime Downloader Tool\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    anime_name = input(\"Enter anime name to search: \").strip()\n",
        "    if not anime_name:\n",
        "        print(\"‚ùå Search query cannot be empty\")\n",
        "        raise SystemExit\n",
        "\n",
        "    print(f\"\\nSearching for '{anime_name}'...\")\n",
        "    search_results = search_anime(anime_name)\n",
        "\n",
        "    # print(search_results)\n",
        "\n",
        "    if search_results.get(\"total\", 0) == 0:\n",
        "        print(\"‚ùå No results found. Try a different search term.\")\n",
        "        raise SystemExit\n",
        "\n",
        "\n",
        "    print(f\"\\nüìã Found {search_results['total']} results:\")\n",
        "    print(\"-\" * 50)\n",
        "    for idx, anime in enumerate(search_results[\"data\"]):\n",
        "        print(f\"[{idx}] {anime['title']} ({anime.get('year', 'N/A')}) ‚Ä¢ {anime.get  ('episodes', '?')} episodes ‚Ä¢ {anime.get('status', 'Unknown')}\")\n",
        "\n",
        "    # Step 2: Select anime from results\n",
        "    while True:\n",
        "        try:\n",
        "            selection = int(input(f\"\\nSelect anime by number [0-{len(search_results ['data'])-1}]: \"))\n",
        "            if 0 <= selection < len(search_results[\"data\"]):\n",
        "                selected_anime = search_results[\"data\"][selection]\n",
        "                break\n",
        "            else:\n",
        "                print(f\"‚ùå Please enter a number between 0 and {len(search_results  ['data'])-1}\")\n",
        "        except ValueError:\n",
        "            print(\"‚ùå Please enter a valid number\")\n",
        "\n",
        "    # Step 3: Fetch series information and episodes\n",
        "    anime_session = selected_anime[\"session\"]\n",
        "    anime_title = selected_anime[\"title\"]\n",
        "    page1_data = get_series_info(anime_session, page=1)\n",
        "    total_episodes = page1_data.get(\"total\", 0)\n",
        "    total_pages = page1_data.get(\"total_pages\", 1)\n",
        "    episodes_per_page = len(page1_data.get(\"episodes\", []))\n",
        "\n",
        "    print(f\"\\nüé¨ {page1_data.get('title', selected_anime['title'])}\")\n",
        "    print(f\"üìä Total Episodes: {total_episodes} (across {total_pages} pages)\")\n",
        "\n",
        "    # Step 4: Select episodes to download\n",
        "    while True:\n",
        "        episode_input = input(\"\\nWhich episodes to download? ('all', single number,     or range like '1-5'): \")\n",
        "        episodes_to_download = validate_episode_range(episode_input, total_episodes)\n",
        "        if episodes_to_download:\n",
        "            break\n",
        "\n",
        "    print(f\"\\nüîÑ Will download {len(episodes_to_download)} episodes:    {episodes_to_download[0]}-{episodes_to_download[-1]}\")\n",
        "\n",
        "    # Step 5: Gather episode session IDs (may need to paginate)\n",
        "    print(\"\\nGathering episode information...\")\n",
        "    episode_sessions = {}\n",
        "    pages_needed = set()\n",
        "\n",
        "    # Determine which pages we need to fetch\n",
        "    for ep_num in episodes_to_download:\n",
        "        page_num = ((ep_num - 1) // episodes_per_page) + 1\n",
        "        pages_needed.add(page_num)\n",
        "\n",
        "    # Fetch each required page and extract session IDs\n",
        "    for page_num in sorted(pages_needed):\n",
        "        print(f\"Fetching page {page_num}/{total_pages}...\")\n",
        "        page_data = get_series_info(anime_session, page=page_num)\n",
        "\n",
        "        for episode in page_data.get(\"episodes\", []):\n",
        "            ep_num = int(episode.get(\"episode\", \"0\"))\n",
        "            if ep_num in episodes_to_download:\n",
        "                episode_sessions[ep_num] = episode.get(\"session\")\n",
        "\n",
        "    # Step 6: Process each episode and get download links\n",
        "    print(\"\\nüîç Processing episodes and fetching download links...\")\n",
        "    download_links = {}\n",
        "\n",
        "    for i, (ep_num, session) in enumerate(tqdm(sorted(episode_sessions.items()),    desc=\"Processing Episodes\")):\n",
        "        try:\n",
        "\n",
        "            episode_data = get_episode_links(anime_session, session)\n",
        "            if not isinstance(episode_data, list) or len(episode_data) == 0:\n",
        "                print(f\"‚ö†Ô∏è No quality links found for Episode {ep_num}\")\n",
        "                continue\n",
        "\n",
        "            best_quality = select_best_quality(episode_data)\n",
        "            if not best_quality:\n",
        "                print(f\"‚ö†Ô∏è No download links found for Episode {ep_num}\")\n",
        "                continue\n",
        "\n",
        "\n",
        "            kwik_url = best_quality.get(\"link\")\n",
        "            ddl_data = resolve_kwik_link(kwik_url)\n",
        "\n",
        "\n",
        "            direct_url = ddl_data.get(\"content\", {}).get(\"url\", \"\")\n",
        "            if not direct_url:\n",
        "                print(f\"‚ö†Ô∏è Empty direct URL for Episode {ep_num}\")\n",
        "                continue\n",
        "\n",
        "\n",
        "            expiry_time = parse_expiry_time(direct_url)\n",
        "            quality = best_quality.get(\"name\", \"Unknown\")\n",
        "            download_links[ep_num] = {\n",
        "                \"url\": direct_url,\n",
        "                \"quality\": quality,\n",
        "                \"expires\": expiry_time\n",
        "            }\n",
        "\n",
        "\n",
        "            if (i + 1) % RATE_LIMIT_BATCH == 0 and i + 1 < len(episode_sessions):\n",
        "                print(f\"üí§ Sleeping for {RATE_LIMIT_DELAY}\")\n",
        "                time.sleep(RATE_LIMIT_DELAY)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error processing Episode {ep_num}: {str(e)}\")\n",
        "\n",
        "    # Step 7: Display results\n",
        "    print(\"\\n‚úÖ Download Links Ready:\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    if not download_links:\n",
        "        print(\"‚ùå No download links were successfully generated.\")\n",
        "    else:\n",
        "        for ep_num in sorted(download_links.keys()):\n",
        "            info = download_links[ep_num]\n",
        "            print(f\"Episode {ep_num:02d} [{info['quality']}]\")\n",
        "            print(f\"üîó {info['url']}\")\n",
        "            print(f\"‚è∞ Expires: {info['expires']}\")\n",
        "            print(\"-\" * 80)\n",
        "\n",
        "        proceed = input(\"\\nDo you want to start downloading these files now? (y/n):     \").strip().lower()\n",
        "        if proceed == 'y':\n",
        "            # Step 8: Download in memory\n",
        "            urls_to_download = [info[\"url\"] for info in download_links.values()]\n",
        "            download_media(urls=urls_to_download , folder = f\"{DOWNLOADS_DIR}/  {anime_title}\")\n",
        "        else:\n",
        "            print(\"Download skipped by user.\")\n",
        "\n",
        "\n",
        "    print(f\"\\nüéâ Successfully generated {len(download_links)} out of {len   (episodes_to_download)} requested episode links.\")"
      ],
      "metadata": {
        "id": "qZrA-CWc-bUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  AnimeKai Downloader (Soon)\n"
      ],
      "metadata": {
        "id": "M95dH6o8i4VR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üìÇüîÑ Manage Downloaded Files & Folders\n",
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import gc\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "from natsort import natsorted\n",
        "\n",
        "\n",
        "def get_folder_size(folder_path: Path) -> int:\n",
        "    return sum(f.stat().st_size for f in folder_path.rglob('*') if f.is_file())\n",
        "\n",
        "def natural_sort_key(p: Path):\n",
        "    return [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\\d+)', p.name)]\n",
        "\n",
        "\n",
        "def list_files() -> dict[str,Path]:\n",
        "    base = Path(DOWNLOADS_DIR)\n",
        "    entries = sorted(base.iterdir(), key=natural_sort_key) if base.exists() else []\n",
        "    if not entries:\n",
        "        print(\"üìÇ No files or folders found.\")\n",
        "        return {}\n",
        "    print(f\"{'Index':<6} {'Name':<50} {'Type':<8} {'Size':<10} {'Modified'}\")\n",
        "    idx_map = {}\n",
        "    def _print(entry: Path, idx: str, level: int):\n",
        "        indent = \"  \"*level\n",
        "        mtime = datetime.fromtimestamp(entry.stat().st_mtime).strftime(\"%Y-%m-%d %H:%M\")\n",
        "        typ = \"File\" if entry.is_file() else \"Folder\"\n",
        "        size = format_size(entry.stat().st_size) if entry.is_file() else format_size(get_folder_size(entry))\n",
        "        name = entry.name + (\"/\" if entry.is_dir() else \"\")\n",
        "        print(f\"{idx:<6} {indent+name:<50} {typ:<8} {size:<10} {mtime}\")\n",
        "        idx_map[idx] = entry\n",
        "        if entry.is_dir():\n",
        "            subs = sorted(entry.iterdir(), key=natural_sort_key)\n",
        "            for i, sub in enumerate(subs):\n",
        "                _print(sub, f\"{idx}.{i}\", level+1)\n",
        "    for i, e in enumerate(entries):\n",
        "        _print(e, str(i), 0)\n",
        "    return idx_map\n",
        "\n",
        "\n",
        "def parse_indices(s: str, keys: set[str]) -> list[str]:\n",
        "    out = set()\n",
        "    for tok in [t.strip() for t in s.split(',') if t.strip()]:\n",
        "        if ':' in tok:\n",
        "            a, b = tok.split(':')\n",
        "            depth = a.count('.')\n",
        "            prefix = a.rsplit('.', 1)[0] + '.' if '.' in a else ''\n",
        "            sibs = natsorted(k for k in keys if k.startswith(prefix) and k.count('.') == depth)\n",
        "            if a in sibs and b in sibs:\n",
        "                ia, ib = sibs.index(a), sibs.index(b)\n",
        "                for k in sibs[min(ia, ib): max(ia, ib)+1]:\n",
        "                    out.add(k)\n",
        "        else:\n",
        "            out.add(tok)\n",
        "    return sorted(out, key=lambda x: [int(t) if t.isdigit() else t for t in x.split('.')])\n",
        "\n",
        "def delete_selected(idx_map: dict[str,Path]):\n",
        "    inp = input(\"\\nüóëÔ∏è Enter index(es) to delete (e.g. 1,2.0:2.3,3.1): \").strip()\n",
        "    if not inp:\n",
        "        print(\"üö´ No input‚Äîaborting.\")\n",
        "        return\n",
        "    chosen = parse_indices(inp, set(idx_map))\n",
        "    to_del = []\n",
        "    for k in chosen:\n",
        "        p = idx_map.get(k)\n",
        "        if p and p.exists():\n",
        "            to_del.append(p)\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è Invalid index: {k}\")\n",
        "    if not to_del:\n",
        "        print(\"üö´ No valid targets.\")\n",
        "        return\n",
        "    print(\"\\n‚ö†Ô∏è Will delete:\")\n",
        "    for p in to_del:\n",
        "        print(\" ‚Ä¢\", p)\n",
        "    if input(\"Type 'yes' to confirm: \").strip().lower() != 'yes':\n",
        "        print(\"üö´ Cancelled.\")\n",
        "        return\n",
        "    for p in to_del:\n",
        "        try:\n",
        "            if p.is_dir(): shutil.rmtree(p)\n",
        "            else: p.unlink()\n",
        "            print(\"‚úÖ Deleted:\", p)\n",
        "        except Exception as e:\n",
        "            print(\"‚ùå Error deleting\", p, \":\", e)\n",
        "    gc.collect()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    index_map = list_files()\n",
        "    if index_map:\n",
        "        if input(\"\\n‚ùì Delete any? (y/N): \").strip().lower() == 'y':\n",
        "            delete_selected(index_map)\n"
      ],
      "metadata": {
        "id": "F_I_XJI0ytmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üîç Show Media Info (mediainfo)\n",
        "\n",
        "import subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def show_media_info():\n",
        "    path = input(\"üéûÔ∏è Enter full path to media file: \").strip()\n",
        "    file = Path(path)\n",
        "\n",
        "    if not file.exists():\n",
        "        print(\"‚ùå File not found.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            [\"mediainfo\", \"--Output=Tree\", str(file)],\n",
        "            check=True,\n",
        "            stdout=subprocess.PIPE,\n",
        "            text=True\n",
        "        )\n",
        "        print(f\"\\nüìÑ Media Info for: {file.name}\\n\")\n",
        "        print(result.stdout)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error running mediainfo: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    show_media_info()\n"
      ],
      "metadata": {
        "id": "W3JO8NpfzdUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üì¶ Zipper (Zip and Split File or Folder)\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "target_name = input(\"üìÇ Enter file/folder path: \").strip()\n",
        "target_path = os.path.join(DOWNLOADS_DIR, target_name)\n",
        "\n",
        "zip_base_name = input(\"üì¶ Enter output base name: \").strip()\n",
        "if not zip_base_name:\n",
        "    zip_base_name = os.path.basename(target_name)\n",
        "\n",
        "if zip_base_name.lower().endswith('.zip'):\n",
        "    zip_base_name = zip_base_name[:-4]\n",
        "\n",
        "\n",
        "full_zip_path = os.path.join(DOWNLOADS_DIR, f\"{zip_base_name}.zip\")\n",
        "split_prefix = os.path.join(DOWNLOADS_DIR, zip_base_name)\n",
        "\n",
        "\n",
        "if not os.path.exists(target_path):\n",
        "    print(\"‚ùå Not found:\", target_path)\n",
        "else:\n",
        "    is_file = os.path.isfile(target_path)\n",
        "    print(\"üìå Detected:\", \"File\" if is_file else \"Folder\")\n",
        "    print(f\"\\nüì¶ Zipping {'file' if is_file else 'folder'} ‚Üí {full_zip_path}\")\n",
        "    if is_file:\n",
        "        !zip -j \"{full_zip_path}\" \"{target_path}\"\n",
        "    else:\n",
        "        !zip -r \"{full_zip_path}\" \"{target_path}\"\n",
        "\n",
        "\n",
        "    do_split = input(\"\\n‚ùì Do you want to split the ZIP? (y/N): \").strip().lower()\n",
        "    if do_split == 'y':\n",
        "        split_size = input(\"üìê Enter max part size (e.g., 4G, 700M) [default: 4G]: \").strip()\n",
        "        if not split_size:\n",
        "            split_size = \"4G\"\n",
        "\n",
        "        print(f\"\\n‚úÇÔ∏è Splitting ZIP into chunks of {split_size} as .partN files...\")\n",
        "        split_cmd = f'split -b {split_size} \"{full_zip_path}\" --numeric-suffixes=1 --suffix-length=1 \"{full_zip_path}.part\"'\n",
        "        os.system(split_cmd)\n",
        "\n",
        "        print(\"\\n‚úÖ Split complete. Output parts:\")\n",
        "        !ls -lh \"{full_zip_path}.part\"*\n",
        "    else:\n",
        "        print(\"\\nüì¶ Skipped splitting. Single ZIP is ready.\")\n"
      ],
      "metadata": {
        "id": "IdDNYLLUz9NH"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}